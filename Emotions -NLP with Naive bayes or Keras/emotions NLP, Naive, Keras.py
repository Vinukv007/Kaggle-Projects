# -*- coding: utf-8 -*-
"""emotions prediction using nlp, naive bayes, keras.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SN_HQd0JDsMh4P6UT8RaDleuSEzJGwPX
"""

import pandas as pd
import numpy as np
import seaborn as sns
import nltk
import matplotlib.pyplot as plt

df=pd.read_csv('train.txt', header=None)

df.columns={'test'}

df['text']=[i.split(';')[0] for i in df.test]

df['emotion']=[i.split(';')[1] for i in df.test]

df=df.drop('test', 1)
df.head()

df.emotion.unique()

df.describe()

df.drop_duplicates(inplace=True)

df.info()

sns.catplot(data=df, y="emotion", kind="count", palette="autumn", )

"""Cleaning the data using NLTK

"""

import re

from nltk.corpus import stopwords
nltk.download('stopwords')
sw=set(stopwords.words('english'))

from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS
sklearn_stopwords = set(ENGLISH_STOP_WORDS)

final_sw=list(sw.union(sklearn_stopwords))

from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
nltk.download('wordnet')
nltk.download('omw-1.4')

pip install contractions

import contractions

def tweet_cleaner_without_stopwords(text):
    new_text = re.sub(r"'s\b", " is", text)
    new_text = re.sub("#", "", new_text)
    new_text = re.sub("@[A-Za-z0-9]+", "", new_text)
    new_text = re.sub(r"http\S+", "", new_text)
    new_text = contractions.fix(new_text)    
    new_text = re.sub(r"[^a-zA-Z]", " ", new_text)    
    new_text = new_text.lower().strip()
    new_text = ' '.join([i for i in new_text.split(' ') if i not in final_sw and len(i)>2])
    
    cleaned_text = ''
    for token in new_text.split():
        cleaned_text = cleaned_text + lemmatizer.lemmatize(token) + ' '
    
    return cleaned_text

df['finaltext']=df.text.apply(tweet_cleaner_without_stopwords)

df.head()

all_words = []
for t in df.text:
    all_words.extend(t.split())

freq_dist = nltk.FreqDist(all_words)

plt.figure(figsize=(12,5))
plt.title('Top 25 most common words')
plt.xticks(fontsize=15)

freq_dist.plot(25, cumulative=False)

plt.show()



all_words_wo_sw = []
count=0
for t in df.finaltext:
    all_words_wo_sw.extend(t.split())

freq_dist = nltk.FreqDist(all_words_wo_sw)

plt.figure(figsize=(12,5))
plt.title('Top 25 most common words')
plt.xticks(fontsize=15)

freq_dist.plot(25, cumulative=False)

plt.show()

"""Fitting model and prediction"""

from sklearn.feature_extraction.text import CountVectorizer
vec=CountVectorizer(ngram_range=(1,2), max_features=1000)

count_sent=vec.fit_transform(df.finaltext)

x=count_sent.toarray()

y=df.emotion

from sklearn.naive_bayes import MultinomialNB
mn=MultinomialNB()

from sklearn.model_selection import train_test_split
xtrain, xtest, ytrain, ytest=train_test_split(x, y, test_size=0.2, random_state=1)

mn.fit(xtrain, ytrain)

print(mn.score(xtrain, ytrain))
print(mn.score(xtest, ytest))

"""testing the model"""

a={'test':'i feel really sad and happy and happy and happy'}
a=pd.DataFrame(data=a, index={'test'})
a.head()

a=vec.transform(a.test)

b=a.toarray()

mn.predict(b)



"""using keras"""

from sklearn.preprocessing import LabelEncoder
lb=LabelEncoder()
y=lb.fit_transform(y)
y.shape

from keras.utils import to_categorical
y=to_categorical(y)

y[0]

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense

len(df.emotion.unique())

model=Sequential()
model.add(Dense(32, activation='relu',input_shape=(1000,)))
model.add(Dense(64,activation='relu'))
model.add(Dense(16,activation='relu'))
model.add(Dense(8,activation='relu'))
model.add(Dense(6, activation='softmax'))

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

from sklearn.model_selection import train_test_split
xtrain, xtest, ytrain, ytest=train_test_split(x, y, test_size=0.2, random_state=1)

data=model.fit(xtrain,ytrain, batch_size=64, epochs=50, validation_data=(xtest, ytest))

# plot loss
plt.subplot(211)
plt.title('Cross Entropy Loss')
plt.plot(data.history['loss'], color='blue', label='train')
plt.plot(data.history['val_loss'], color='orange', label='test')
# plot accuracy
plt.subplot(212)
plt.title('Classification Accuracy')
plt.plot(data.history['accuracy'], color='blue', label='train')
plt.plot(data.history['val_accuracy'], color='orange', label='test')

model.evaluate(xtest, ytest)

#Different optimizer

from keras.optimizers import SGD
model=Sequential()
model.add(Dense(32, activation='relu',input_shape=(1000,)))
model.add(Dense(64,activation='relu',kernel_initializer='he_uniform'))
model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))
model.add(Dense(6, activation='softmax'))
 # compile model
opt = SGD(lr=0.001, momentum=0.9)
model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])

data=model.fit(xtrain,ytrain, batch_size=64, epochs=50, validation_data=(xtest, ytest))

# plot loss
plt.subplot(211)
plt.title('Cross Entropy Loss')
plt.plot(data.history['loss'], color='blue', label='train')
plt.plot(data.history['val_loss'], color='orange', label='test')
# plot accuracy
plt.subplot(212)
plt.title('Classification Accuracy')
plt.plot(data.history['accuracy'], color='blue', label='train')
plt.plot(data.history['val_accuracy'], color='orange', label='test')

model.evaluate(xtest, ytest)

from keras.layers import Dropout
from keras.layers import BatchNormalization

# Batch Optimization

model=Sequential()
model.add(Dense(32, activation='relu',input_shape=(1000,)))
model.add(Dense(64,activation='relu',kernel_initializer='he_uniform'))
model.add(BatchNormalization())
model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))
model.add(BatchNormalization())
model.add(Dense(6, activation='softmax'))
 # compile model
opt = SGD(lr=0.001, momentum=0.9)
model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])

data=model.fit(xtrain,ytrain, batch_size=64, epochs=50, validation_data=(xtest, ytest))

# plot loss
plt.subplot(211)
plt.title('Cross Entropy Loss')
plt.plot(data.history['loss'], color='blue', label='train')
plt.plot(data.history['val_loss'], color='orange', label='test')
# plot accuracy
plt.subplot(212)
plt.title('Classification Accuracy')
plt.plot(data.history['accuracy'], color='blue', label='train')
plt.plot(data.history['val_accuracy'], color='orange', label='test')

model.evaluate(xtest, ytest)

#Dropout

model=Sequential()
model.add(Dense(32, activation='relu',input_shape=(1000,)))
model.add(Dense(64,activation='relu',kernel_initializer='he_uniform'))
model.add(Dropout(0.3))
model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))
model.add(Dropout(0.5))
model.add(Dense(6, activation='softmax'))
 # compile model
opt = SGD(lr=0.001, momentum=0.9)
model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])

data=model.fit(xtrain,ytrain, batch_size=64, epochs=50, validation_data=(xtest, ytest))

# plot loss
plt.subplot(211)
plt.title('Cross Entropy Loss')
plt.plot(data.history['loss'], color='blue', label='train')
plt.plot(data.history['val_loss'], color='orange', label='test')
# plot accuracy
plt.subplot(212)
plt.title('Classification Accuracy')
plt.plot(data.history['accuracy'], color='blue', label='train')
plt.plot(data.history['val_accuracy'], color='orange', label='test')

model.evaluate(xtest, ytest)

#Weight Decay

from keras.regularizers import l2

model=Sequential()
model.add(Dense(32, activation='relu',input_shape=(1000,)))
model.add(Dense(64,activation='relu',kernel_initializer='he_uniform',kernel_regularizer=l2(0.001)))
model.add(Dense(128, activation='relu', kernel_initializer='he_uniform',kernel_regularizer=l2(0.001)))
model.add(Dense(128, activation='relu', kernel_initializer='he_uniform',kernel_regularizer=l2(0.001)))
model.add(Dense(6, activation='softmax'))
 # compile model
opt = SGD(lr=0.001, momentum=0.9)
model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])

data=model.fit(xtrain,ytrain, batch_size=64, epochs=50, validation_data=(xtest, ytest))

# plot loss
plt.subplot(211)
plt.title('Cross Entropy Loss')
plt.plot(data.history['loss'], color='blue', label='train')
plt.plot(data.history['val_loss'], color='orange', label='test')
# plot accuracy
plt.subplot(212)
plt.title('Classification Accuracy')
plt.plot(data.history['accuracy'], color='blue', label='train')
plt.plot(data.history['val_accuracy'], color='orange', label='test')

model.evaluate(xtest, ytest)

# choosing the dropout model

model=Sequential()
model.add(Dense(32, activation='relu',input_shape=(1000,)))
model.add(Dense(64,activation='relu',kernel_initializer='he_uniform'))
model.add(Dropout(0.3))
model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))
model.add(Dropout(0.5))
model.add(Dense(6, activation='softmax'))
 # compile model
opt = SGD(lr=0.001, momentum=0.9)
model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])

data=model.fit(xtrain,ytrain, batch_size=64, epochs=50, validation_data=(xtest, ytest))

# plot loss
plt.subplot(211)
plt.title('Cross Entropy Loss')
plt.plot(data.history['loss'], color='blue', label='train')
plt.plot(data.history['val_loss'], color='orange', label='test')
# plot accuracy
plt.subplot(212)
plt.title('Classification Accuracy')
plt.plot(data.history['accuracy'], color='blue', label='train')
plt.plot(data.history['val_accuracy'], color='orange', label='test')

model.evaluate(xtest, ytest)